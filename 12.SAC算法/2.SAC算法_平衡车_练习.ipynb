{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.01897067, -0.04997724, -0.00336339, -0.018354  ], dtype=float32)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "\n",
    "#定义环境\n",
    "class MyWrapper(gym.Wrapper):\n",
    "\n",
    "    def __init__(self):\n",
    "        env = gym.make('CartPole-v1', render_mode='rgb_array')\n",
    "        super().__init__(env)\n",
    "        self.env = env\n",
    "        self.step_n = 0\n",
    "\n",
    "    def reset(self):\n",
    "        state, _ = self.env.reset()\n",
    "        self.step_n = 0\n",
    "        return state\n",
    "\n",
    "    def step(self, action):\n",
    "        state, reward, terminated, truncated, info = self.env.step(action)\n",
    "        done = terminated or truncated\n",
    "        self.step_n += 1\n",
    "        if self.step_n >= 200:\n",
    "            done = True\n",
    "        return state, reward, done, info\n",
    "\n",
    "\n",
    "env = MyWrapper()\n",
    "\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "#打印游戏\n",
    "def show():\n",
    "    plt.imshow(env.render())\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0, 1],\n",
       " tensor([[0.5062],\n",
       "         [0.9233]], grad_fn=<ViewBackward0>))"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import random\n",
    "\n",
    "\n",
    "class ModelAction(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = torch.nn.Sequential(\n",
    "                    torch.nn.Linear(4, 128),\n",
    "                    torch.nn.ReLU(),\n",
    "                    torch.nn.Linear(128, 2),\n",
    "                    torch.nn.Softmax(dim=1),\n",
    "                )\n",
    "\n",
    "    def forward(self, state):\n",
    "        prob = self.model(state)  # 输出概率 [1, 2]\n",
    "\n",
    "        # 采样动作\n",
    "        action = [random.choices(range(2), weights=p.tolist(), k=1)[0] for p in prob]  # 采样一个动作\n",
    "\n",
    "        # 计算熵\n",
    "        # prob[0, action] 是采样动作 action 的概率\n",
    "        log_prob = torch.log(prob[0, action] + 1e-7)  # 避免 log(0) 加小 epsilon\n",
    "        entropy = -log_prob  # 单个动作的熵贡献，-log π(a|s)\n",
    "\n",
    "        # 或者更准确的熵估计（基于整个分布）\n",
    "        # 熵 = -∑ π(a|s) log π(a|s)，可以使用 prob 直接计算期望\n",
    "        #entropy = -torch.sum(prob * torch.log(prob + 1e-7))  # 整个分布的熵\n",
    "\n",
    "        return action, entropy.reshape(-1,1)\n",
    "\n",
    "\n",
    "model_action = ModelAction()\n",
    "\n",
    "model_action(torch.randn(2,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1072],\n",
       "        [-0.0345]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class ModelValue(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.sequential = torch.nn.Sequential(\n",
    "            torch.nn.Linear(5, 128),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(128, 128),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(128, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        #[b, 4+1] -> [b, 5]\n",
    "        state = torch.cat([state, action], dim=1)\n",
    "\n",
    "        #[b, 5] -> [b, 1]\n",
    "        return self.sequential(state)\n",
    "\n",
    "\n",
    "model_value1 = ModelValue()\n",
    "model_value2 = ModelValue()\n",
    "\n",
    "model_value_next1 = ModelValue()\n",
    "model_value_next2 = ModelValue()\n",
    "\n",
    "model_value_next1.load_state_dict(model_value1.state_dict())\n",
    "model_value_next2.load_state_dict(model_value2.state_dict())\n",
    "\n",
    "model_value1(torch.randn(2, 4), torch.randn(2, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def get_action(state):\n",
    "    state = torch.FloatTensor(state).reshape(1, 4)\n",
    "    #[1, 4] -> [1, 2]\n",
    "    action, _ = model_action(state)\n",
    "\n",
    "    return action\n",
    "\n",
    "\n",
    "get_action([1, 2, 3, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((206, 0), 206)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#样本池\n",
    "datas = []\n",
    "\n",
    "\n",
    "### 改1：要收集足够多的数据\n",
    "#向样本池中添加N条数据,删除M条最古老的数据\n",
    "def update_data():\n",
    "    old_count = len(datas)\n",
    "\n",
    "    #玩到新增了N个数据为止\n",
    "    while len(datas) - old_count < 200:\n",
    "        #初始化游戏\n",
    "        state = env.reset()\n",
    "\n",
    "        #玩到游戏结束为止\n",
    "        over = False\n",
    "        while not over:\n",
    "            #根据当前状态得到一个动作\n",
    "            action = get_action(state)\n",
    "\n",
    "            #执行动作,得到反馈\n",
    "            next_state, reward, over, _ = env.step(action[0])\n",
    "\n",
    "            #记录数据样本\n",
    "            datas.append((state, action, reward, next_state, over))\n",
    "\n",
    "            #更新游戏状态,开始下一个动作\n",
    "            state = next_state\n",
    "\n",
    "    update_count = len(datas) - old_count\n",
    "    drop_count = max(len(datas) - 10000, 0)\n",
    "\n",
    "    #数据上限,超出时从最古老的开始删除\n",
    "    while len(datas) > 10000:\n",
    "        datas.pop(0)\n",
    "\n",
    "    return update_count, drop_count\n",
    "\n",
    "\n",
    "update_data(), len(datas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_450601/2486169052.py:11: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  reward = torch.LongTensor([i[2] for i in samples]).reshape(-1, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.0302,  0.6290, -0.0311, -1.0223],\n",
       "         [ 0.0159,  0.0433, -0.0054, -0.1349],\n",
       "         [ 0.0189, -0.1520, -0.0086,  0.1605],\n",
       "         [ 0.0437, -0.3484, -0.0426,  0.4827],\n",
       "         [ 0.1225,  1.2214, -0.1881, -2.0840]]),\n",
       " tensor([[0.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [0.]]),\n",
       " tensor([[1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1]]),\n",
       " tensor([[ 0.0428,  0.4344, -0.0516, -0.7396],\n",
       "         [ 0.0168,  0.2385, -0.0081, -0.4292],\n",
       "         [ 0.0159,  0.0433, -0.0054, -0.1349],\n",
       "         [ 0.0367, -0.5429, -0.0330,  0.7616],\n",
       "         [ 0.1469,  1.0286, -0.2298, -1.8549]]),\n",
       " tensor([[0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [1]]))"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#获取一批数据样本\n",
    "def get_sample():\n",
    "    #从样本池中采样\n",
    "    samples = random.sample(datas, 64)\n",
    "\n",
    "    #[b, 4]\n",
    "    state = torch.FloatTensor([i[0] for i in samples]).reshape(-1, 4)\n",
    "    #[b, 1]\n",
    "    action = torch.FloatTensor([i[1] for i in samples]).reshape(-1, 1)\n",
    "    #[b, 1]\n",
    "    reward = torch.LongTensor([i[2] for i in samples]).reshape(-1, 1)\n",
    "    #[b, 4]\n",
    "    next_state = torch.FloatTensor([i[3] for i in samples]).reshape(-1, 4)\n",
    "    #[b, 1]\n",
    "    over = torch.LongTensor([i[4] for i in samples]).reshape(-1, 1)\n",
    "\n",
    "    return state, action, reward, next_state, over\n",
    "\n",
    "\n",
    "state, action, reward, next_state, over = get_sample()\n",
    "\n",
    "state[:5], action[:5], reward[:5], next_state[:5], over[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16.0"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython import display\n",
    "\n",
    "\n",
    "def test(play):\n",
    "    #初始化游戏\n",
    "    state = env.reset()\n",
    "\n",
    "    #记录反馈值的和,这个值越大越好\n",
    "    reward_sum = 0\n",
    "\n",
    "    #玩到游戏结束为止\n",
    "    over = False\n",
    "    while not over:\n",
    "        #根据当前状态得到一个动作\n",
    "        action = get_action(state)\n",
    "\n",
    "        #执行动作,得到反馈\n",
    "        state, reward, over, _ = env.step(action[0])\n",
    "        reward_sum += reward\n",
    "\n",
    "        #打印动画\n",
    "        if play and random.random() < 0.2:  #跳帧\n",
    "            display.clear_output(wait=True)\n",
    "            show()\n",
    "\n",
    "    return reward_sum\n",
    "\n",
    "\n",
    "test(play=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def soft_update(model, model_next):\n",
    "    for param, param_next in zip(model.parameters(), model_next.parameters()):\n",
    "        #以一个小的比例更新\n",
    "        value = param_next.data * 0.995 + param.data * 0.005\n",
    "        param_next.data.copy_(value)\n",
    "\n",
    "\n",
    "soft_update(torch.nn.Linear(4, 64), torch.nn.Linear(4, 64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-4.6052, requires_grad=True)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "#这也是一个可学习的参数\n",
    "alpha = torch.tensor(math.log(0.01))\n",
    "alpha.requires_grad = True\n",
    "\n",
    "alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 1])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_target(reward, next_state, over):\n",
    "    #首先使用model_action计算动作和动作的熵\n",
    "    #[b, 4] -> [b, 1],[b, 1]\n",
    "    action, entropy = model_action(next_state)\n",
    "\n",
    "    #评估next_state的价值\n",
    "    #[b, 4],[b, 1] -> [b, 1]\n",
    "    action_tensor = torch.tensor(action, dtype=torch.float32).reshape(-1,1)\n",
    "    target1 = model_value_next1(next_state, action_tensor)\n",
    "    target2 = model_value_next2(next_state, action_tensor)\n",
    "\n",
    "    #取价值小的,这是出于稳定性考虑\n",
    "    #[b, 1]\n",
    "    target = torch.min(target1, target2)\n",
    "\n",
    "    #exp和log互为反操作,这里是把alpha还原了\n",
    "    #这里的操作是在target上加上了动作的熵,alpha作为权重系数\n",
    "    #[b, 1] - [b, 1] -> [b, 1]\n",
    "    target += alpha.exp() * entropy\n",
    "\n",
    "    #[b, 1]\n",
    "    target *= 0.99\n",
    "    target *= (1 - over)\n",
    "    target += reward\n",
    "\n",
    "    return target\n",
    "\n",
    "\n",
    "get_target(reward, next_state, over).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-0.0124, grad_fn=<MeanBackward0>),\n",
       " tensor([[0.6045],\n",
       "         [0.6045],\n",
       "         [0.6045],\n",
       "         [0.7904],\n",
       "         [0.6045],\n",
       "         [0.6045],\n",
       "         [0.6045],\n",
       "         [0.7904],\n",
       "         [0.7904],\n",
       "         [0.7904],\n",
       "         [0.7904],\n",
       "         [0.7904],\n",
       "         [0.6045],\n",
       "         [0.7904],\n",
       "         [0.7904],\n",
       "         [0.7904],\n",
       "         [0.7904],\n",
       "         [0.7904],\n",
       "         [0.6045],\n",
       "         [0.6045]], grad_fn=<ViewBackward0>))"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_loss_action(state):\n",
    "    #计算action和熵\n",
    "    #[b, 4] -> [b, 1],[b, 1]\n",
    "    action, entropy = model_action(state)\n",
    "\n",
    "    #使用两个value网络评估action的价值\n",
    "    #[b, 4],[b, 1] -> [b, 1]\n",
    "    action_tensor = torch.tensor(action, dtype=torch.float32).reshape(-1,1)\n",
    "    value1 = model_value1(state, action_tensor)\n",
    "    value2 = model_value2(state, action_tensor)\n",
    "\n",
    "    #取价值小的,出于稳定性考虑\n",
    "    #[b, 1]\n",
    "    value = torch.min(value1, value2)\n",
    "\n",
    "    #alpha还原后乘以熵,这个值期望的是越大越好,但是这里是计算loss,所以符号取反\n",
    "    #[1] - [b, 1] -> [b, 1]\n",
    "    loss_action = -alpha.exp() * entropy\n",
    "\n",
    "    #减去value,所以value越大越好,这样loss就会越小\n",
    "    loss_action -= value #图片公式里用了V，这里没用到\n",
    "\n",
    "    return loss_action.mean(), entropy\n",
    "\n",
    "\n",
    "get_loss_action(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_450601/2486169052.py:11: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  reward = torch.LongTensor([i[2] for i in samples]).reshape(-1, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 51 0.008872825652360916 11.2\n",
      "10 142 0.004899808205664158 9.3\n",
      "20 232 0.0030080070719122887 9.5\n",
      "30 327 0.001874864916317165 9.3\n",
      "40 424 0.0012230564607307315 9.5\n",
      "50 517 0.0008187643834389746 9.1\n",
      "60 610 0.0005576490657404065 9.5\n",
      "70 707 0.000384301645681262 9.3\n",
      "80 801 0.0002663575578480959 9.4\n",
      "90 896 0.00018733018077909946 9.3\n"
     ]
    }
   ],
   "source": [
    "def train():\n",
    "    optimizer_action = torch.optim.Adam(model_action.parameters(), lr=3e-4)\n",
    "    optimizer_value1 = torch.optim.Adam(model_value1.parameters(), lr=3e-3)\n",
    "    optimizer_value2 = torch.optim.Adam(model_value2.parameters(), lr=3e-3)\n",
    "\n",
    "    #alpha也是要更新的参数,所以这里要定义优化器\n",
    "    optimizer_alpha = torch.optim.Adam([alpha], lr=3e-4)\n",
    "\n",
    "    loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "    #训练N次\n",
    "    for epoch in range(100):\n",
    "        #更新N条数据\n",
    "        update_data()\n",
    "\n",
    "        #每次更新过数据后,学习N次\n",
    "        for i in range(200):\n",
    "            #采样一批数据\n",
    "            state, action, reward, next_state, over = get_sample()\n",
    "\n",
    "            #对reward偏移,为了便于训练\n",
    "            reward = (reward + 8) / 8\n",
    "\n",
    "            #计算target,这个target里已经考虑了动作的熵\n",
    "            #[b, 1]\n",
    "            target = get_target(reward, next_state, over)\n",
    "            target = target.detach()\n",
    "\n",
    "            #计算两个value\n",
    "            value1 = model_value1(state, action)\n",
    "            value2 = model_value2(state, action)\n",
    "\n",
    "            #计算两个loss,两个value的目标都是要贴近target\n",
    "            loss_value1 = loss_fn(value1, target)\n",
    "            loss_value2 = loss_fn(value2, target)\n",
    "\n",
    "            #更新参数\n",
    "            optimizer_value1.zero_grad()\n",
    "            loss_value1.backward()\n",
    "            optimizer_value1.step()\n",
    "\n",
    "            optimizer_value2.zero_grad()\n",
    "            loss_value2.backward()\n",
    "            optimizer_value2.step()\n",
    "\n",
    "            #使用model_value计算model_action的loss\n",
    "            loss_action, entropy = get_loss_action(state)\n",
    "            optimizer_action.zero_grad()\n",
    "            loss_action.backward()\n",
    "            optimizer_action.step()\n",
    "\n",
    "            #熵乘以alpha就是alpha的loss\n",
    "            #[b, 1] -> [1]\n",
    "            loss_alpha = (entropy + 1).detach() * alpha.exp()\n",
    "            loss_alpha = loss_alpha.mean()\n",
    "\n",
    "            #更新alpha值\n",
    "            optimizer_alpha.zero_grad()\n",
    "            loss_alpha.backward()\n",
    "            optimizer_alpha.step()\n",
    "\n",
    "            #增量更新next模型\n",
    "            soft_update(model_value1, model_value_next1)\n",
    "            soft_update(model_value2, model_value_next2)\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            test_result = sum([test(play=False) for _ in range(10)]) / 10\n",
    "            print(epoch, len(datas), alpha.exp().item(), test_result)\n",
    "\n",
    "\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL-py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
