{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.04114848, -0.02758827, -0.00500454, -0.02935486], dtype=float32)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "\n",
    "#定义环境\n",
    "class MyWrapper(gym.Wrapper):\n",
    "\n",
    "    def __init__(self):\n",
    "        env = gym.make('CartPole-v1', render_mode='rgb_array')\n",
    "        super().__init__(env)\n",
    "        self.env = env\n",
    "        self.step_n = 0\n",
    "\n",
    "    def reset(self):\n",
    "        state, _ = self.env.reset()\n",
    "        self.step_n = 0\n",
    "        return state\n",
    "\n",
    "    def step(self, action):\n",
    "        state, reward, terminated, truncated, info = self.env.step(action)\n",
    "        done = terminated or truncated\n",
    "        self.step_n += 1\n",
    "        if self.step_n >= 200:\n",
    "            done = True\n",
    "        return state, reward, done, info\n",
    "\n",
    "\n",
    "env = MyWrapper()\n",
    "\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "#打印游戏\n",
    "def show():\n",
    "    plt.imshow(env.render())\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "改2：离散熵用求和\n",
    "代码位置：ModelAction 类中，计算动作熵的方式从单一动作的负对数概率改为基于整个动作分布的熵求和。\n",
    "修改意义：\n",
    "\n",
    "背景：SAC算法的核心是最大化策略的熵，以鼓励探索。熵的计算需要考虑整个动作分布的概率，而不仅仅是采样动作的概率。离散动作空间的熵公式为：$ H(\\pi) = -\\sum_a \\pi(a|s) \\log \\pi(a|s) $。\n",
    "作用：\n",
    "\n",
    "原代码仅计算采样动作的熵（entropy = -log_prob），忽略了动作分布的整体特性，可能导致熵估计偏低，探索不足。\n",
    "修改后使用整个动作分布的熵公式（entropy = -prob * torch.log(prob + 1e-8).sum(dim=1)），更准确地反映了策略的随机性，符合SAC算法的熵正则化目标。\n",
    "\n",
    "\n",
    "效果：更准确的熵计算增强了探索能力，使策略更倾向于尝试不同动作，从而避免过早收敛到次优解。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([1, 1],\n",
       " tensor([[0.6677],\n",
       "         [0.6200]], grad_fn=<ViewBackward0>),\n",
       " tensor([[0.3878, 0.6122],\n",
       "         [0.3111, 0.6889]], grad_fn=<SoftmaxBackward0>))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import random\n",
    "\n",
    "\n",
    "class ModelAction(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = torch.nn.Sequential(\n",
    "                    torch.nn.Linear(4, 128),\n",
    "                    torch.nn.ReLU(),\n",
    "                    torch.nn.Linear(128, 2),\n",
    "                    torch.nn.Softmax(dim=1),\n",
    "                )\n",
    "\n",
    "    def forward(self, state):\n",
    "        prob = self.model(state)  \n",
    "\n",
    "        # 采样动作\n",
    "        action = [random.choices(range(2), weights=p.tolist(), k=1)[0] for p in prob]  # 采样一个动作\n",
    "\n",
    "        # 计算熵\n",
    "        # prob[0, action] 是采样动作 action 的概率\n",
    "        #log_prob = torch.log(prob + 1e-7)  # 避免 log(0) 加小 epsilon\n",
    "        #entropy = -log_prob  # 单个动作的熵贡献，-log π(a|s)\n",
    "\n",
    "        # 改2：离散熵用求和\n",
    "        #[b, 2]\n",
    "        entropy = prob * torch.log(prob + 1e-8)\n",
    "\n",
    "        #所有动作的熵求和\n",
    "        #[b, 2] -> [b, 1]\n",
    "        entropy = -entropy.sum(dim=1, keepdim=True)\n",
    "        # 或者更准确的熵估计（基于整个分布）\n",
    "        # 熵 = -∑ π(a|s) log π(a|s)，可以使用 prob 直接计算期望\n",
    "        #entropy = -torch.sum(prob * torch.log(prob + 1e-7))  # 整个分布的熵\n",
    "\n",
    "        return action, entropy.reshape(-1,1), prob\n",
    "\n",
    "\n",
    "model_action = ModelAction()\n",
    "\n",
    "model_action(torch.randn(2,4))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "改4：输出是二维以匹配后面的与prob相乘\n",
    "代码位置：ModelValue 类中，价值网络输出从1维改为2维。\n",
    "修改意义：\n",
    "背景：在离散动作空间的SAC中，价值网络需要为每个动作输出一个Q值（对应于动作空间的维度）。CartPole-v1的动作空间是离散的（**2个动作：左或右**），因此价值网络的输出需要是2维，以匹配动作概率的维度。\n",
    "作用：\n",
    "原代码中价值网络输出可能是1维，无法与动作概率（2维）进行点乘或加权操作。\n",
    "修改后，价值网络输出2维Q值（对应两个动作），可以与动作概率分布（prob）逐元素相乘，计算期望价值。\n",
    "效果：这一修改使价值网络的输出与动作概率分布兼容，保证了策略损失和价值损失的计算正确性。\n",
    "\n",
    "\n",
    "改9：删去一层\n",
    "代码位置：ModelValue 类中，删去价值网络中的一层隐藏层。\n",
    "修改意义：\n",
    "背景：神经网络的深度和复杂度需要与任务的复杂性匹配。CartPole-v1是一个相对简单的环境，过深的网络可能导致过拟合或训练困难。\n",
    "作用：\n",
    "删除一层隐藏层（从多层简化为两层：Linear(4, 128) -> ReLU -> Linear(128, 2)），降低了价值网络的复杂度，减少了过拟合风险。\n",
    "简化网络结构可能使梯度传播更稳定，适合CartPole-v1的简单状态空间（4维）。\n",
    "效果：减少网络复杂性提高了训练效率和稳定性，适合任务需求。\n",
    "**实际运行时，多加的这一层反而导致训练效果变差**\n",
    "\n",
    "改10：去掉动作输入\n",
    "代码位置：ModelValue 类中，去掉动作输入，使价值网络仅接受状态输入。\n",
    "修改意义：\n",
    "背景：SAC算法中，价值网络（Q网络）通常为每个动作输出一个Q值（即$ Q(s,a) $），但在离散动作空间中，Q网络可以设计为仅接受状态输入，输出所有动作的Q值向量。这种设计在CartPole-v1（动作空间为2）中是可行的。\n",
    "作用：\n",
    "原代码中，价值网络将状态和动作拼接作为输入（例如输入维度为5，可能包含4维状态+1维动作）。这要求网络为每个状态-动作对单独预测Q值，增加了模型复杂度，且可能引入不必要的噪声，因为动作信息可以通过输出维度隐式建模。\n",
    "修改后，价值网络仅接受4维状态输入，输出2维Q值（对应两个动作），简化了网络结构，直接预测所有动作的Q值。\n",
    "效果：这一修改降低了模型的输入复杂性，使价值网络更专注于学习状态到Q值的映射，减少了过拟合风险，提高了训练稳定性。\n",
    "**加上动作输入后，训练效果其差**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0095, -0.0516],\n",
       "        [ 0.0274, -0.1135]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class ModelValue(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.sequential = torch.nn.Sequential(\n",
    "            torch.nn.Linear(4, 128),#改10：去掉动作输入\n",
    "            torch.nn.ReLU(),\n",
    "            #torch.nn.Linear(128, 128),\n",
    "            #torch.nn.ReLU(),#改9：删去一层\n",
    "            # 改4：输出是二维以匹配后面的与prob相乘\n",
    "            torch.nn.Linear(128, 2),\n",
    "        )\n",
    "\n",
    "    def forward(self, state):\n",
    "        #[b, 5] -> [b, 1]\n",
    "        return self.sequential(state)\n",
    "\n",
    "\n",
    "model_value1 = ModelValue()\n",
    "model_value2 = ModelValue()\n",
    "\n",
    "model_value_next1 = ModelValue()\n",
    "model_value_next2 = ModelValue()\n",
    "\n",
    "model_value_next1.load_state_dict(model_value1.state_dict())\n",
    "model_value_next2.load_state_dict(model_value2.state_dict())\n",
    "\n",
    "model_value1(torch.randn(2, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def get_action(state):\n",
    "    state = torch.FloatTensor(state).reshape(1, 4)\n",
    "    #[1, 4] -> [1, 2]\n",
    "    action, _, __ = model_action(state)\n",
    "\n",
    "    return action\n",
    "\n",
    "\n",
    "get_action([1, 2, 3, 4])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "改1：要收集足够多的数据\n",
    "代码位置：update_data 函数中，循环收集至少200条数据，并限制样本池大小为10000。\n",
    "\n",
    "修改意义：\n",
    "\n",
    "背景：SAC是一种基于经验回放（Replay Buffer）的强化学习算法，需要从环境中收集足够多的数据样本以保证策略和价值函数的训练稳定性。样本池过小会导致模型学习到的策略缺乏多样性，容易陷入局部最优。\n",
    "作用：\n",
    "通过确保每次更新至少收集200条新数据，增加了数据的多样性，有助于模型探索不同的状态-动作对，改善策略的泛化能力。\n",
    "限制样本池最大为10000条，避免内存占用过大，同时通过删除最旧的数据保持样本的新鲜度，适应环境动态变化。\n",
    "效果：这一修改保证了训练过程中有足够的数据支持，减少了过拟合的风险，提升了模型的稳定性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yuchen/Fdisk/miniconda3/envs/RL-py39/lib/python3.9/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((201, 0), 201)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#样本池\n",
    "datas = []\n",
    "\n",
    "\n",
    "### 改1：要收集足够多的数据\n",
    "#向样本池中添加N条数据,删除M条最古老的数据\n",
    "def update_data():\n",
    "    old_count = len(datas)\n",
    "\n",
    "    #玩到新增了N个数据为止\n",
    "    while len(datas) - old_count < 200:\n",
    "        #初始化游戏\n",
    "        state = env.reset()\n",
    "\n",
    "        #玩到游戏结束为止\n",
    "        over = False\n",
    "        while not over:\n",
    "            #根据当前状态得到一个动作\n",
    "            action = get_action(state)\n",
    "\n",
    "            #执行动作,得到反馈\n",
    "            next_state, reward, over, _ = env.step(action[0])\n",
    "\n",
    "            #记录数据样本\n",
    "            datas.append((state, action, reward, next_state, over))\n",
    "\n",
    "            #更新游戏状态,开始下一个动作\n",
    "            state = next_state\n",
    "\n",
    "    update_count = len(datas) - old_count\n",
    "    drop_count = max(len(datas) - 10000, 0)\n",
    "\n",
    "    #数据上限,超出时从最古老的开始删除\n",
    "    while len(datas) > 10000:\n",
    "        datas.pop(0)\n",
    "\n",
    "    return update_count, drop_count\n",
    "\n",
    "\n",
    "update_data(), len(datas)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "改8： 返回数据类型 action F->L reward:L->F\n",
    "类型没改好，动作只有0,1所以是Long整形，reward是奖励用浮点数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_461985/3749404182.py:8: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)\n",
      "  state = torch.FloatTensor([i[0] for i in samples]).reshape(-1, 4)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.1215, -0.4311,  0.0837,  0.6112],\n",
       "         [ 0.0460, -0.0319, -0.0398,  0.0421],\n",
       "         [ 0.0215,  0.7482, -0.0409, -1.1462],\n",
       "         [-0.0079,  1.1196,  0.0598, -1.1602],\n",
       "         [ 0.0582,  0.3375,  0.0038,  0.0488]]),\n",
       " tensor([[1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [0],\n",
       "         [1]]),\n",
       " tensor([[1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.]]),\n",
       " tensor([[-0.1301, -0.2373,  0.0959,  0.3461],\n",
       "         [ 0.0454,  0.1638, -0.0390, -0.2629],\n",
       "         [ 0.0364,  0.9438, -0.0638, -1.4514],\n",
       "         [ 0.0145,  0.9238,  0.0366, -0.8494],\n",
       "         [ 0.0649,  0.5326,  0.0047, -0.2427]]),\n",
       " tensor([[0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0]]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#获取一批数据样本\n",
    "def get_sample():\n",
    "    #从样本池中采样\n",
    "    samples = random.sample(datas, 64)\n",
    "\n",
    "    # 改8： 返回数据类型 action F->L reward:L->F\n",
    "    #[b, 4]\n",
    "    state = torch.FloatTensor([i[0] for i in samples]).reshape(-1, 4)\n",
    "    #[b, 1]\n",
    "    action = torch.LongTensor([i[1] for i in samples]).reshape(-1, 1)\n",
    "    #[b, 1]\n",
    "    reward = torch.FloatTensor([i[2] for i in samples]).reshape(-1, 1)\n",
    "    #[b, 4]\n",
    "    next_state = torch.FloatTensor([i[3] for i in samples]).reshape(-1, 4)\n",
    "    #[b, 1]\n",
    "    over = torch.LongTensor([i[4] for i in samples]).reshape(-1, 1)\n",
    "\n",
    "    return state, action, reward, next_state, over\n",
    "\n",
    "\n",
    "state, action, reward, next_state, over = get_sample()\n",
    "\n",
    "state[:5], action[:5], reward[:5], next_state[:5], over[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython import display\n",
    "\n",
    "\n",
    "def test(play):\n",
    "    #初始化游戏\n",
    "    state = env.reset()\n",
    "\n",
    "    #记录反馈值的和,这个值越大越好\n",
    "    reward_sum = 0\n",
    "\n",
    "    #玩到游戏结束为止\n",
    "    over = False\n",
    "    while not over:\n",
    "        #根据当前状态得到一个动作\n",
    "        action = get_action(state)\n",
    "\n",
    "        #执行动作,得到反馈\n",
    "        state, reward, over, _ = env.step(action[0])\n",
    "        reward_sum += reward\n",
    "\n",
    "        #打印动画\n",
    "        if play and random.random() < 0.2:  #跳帧\n",
    "            display.clear_output(wait=True)\n",
    "            show()\n",
    "\n",
    "    return reward_sum\n",
    "\n",
    "\n",
    "test(play=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def soft_update(model, model_next):\n",
    "    for param, param_next in zip(model.parameters(), model_next.parameters()):\n",
    "        #以一个小的比例更新\n",
    "        value = param_next.data * 0.995 + param.data * 0.005\n",
    "        param_next.data.copy_(value)\n",
    "\n",
    "\n",
    "soft_update(torch.nn.Linear(4, 64), torch.nn.Linear(4, 64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-4.6052, requires_grad=True)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "#这也是一个可学习的参数\n",
    "alpha = torch.tensor(math.log(0.01))\n",
    "alpha.requires_grad = True\n",
    "\n",
    "alpha"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "改5：二维的value和prob结合求其期望\n",
    "代码位置：get_target 函数中，target价值按动作概率加权求和。\n",
    "修改意义：\n",
    "\n",
    "背景：SAC算法的目标价值（target）需要考虑下一状态的动作分布，计算期望Q值并加上熵正则化项。公式为：$ \\mathbb{E}_{\\pi(a'|s')}[Q(s',a') - \\alpha \\log \\pi(a'|s')] $。\n",
    "作用：\n",
    "\n",
    "原代码直接使用单一动作的Q值，忽略了动作分布的期望。\n",
    "修改后，通过将价值（target）与动作概率（prob）相乘并求和，计算下一状态的期望Q值，并加上熵项（target += alpha.exp() * entropy）。\n",
    "\n",
    "\n",
    "效果：这一修改确保了目标价值的计算符合SAC算法的理论框架，增强了训练的稳定性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 1])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_target(reward, next_state, over):\n",
    "    #首先使用model_action计算动作和动作的熵\n",
    "    #[b, 4] -> [b, 1],[b, 1]\n",
    "    action, entropy, prob= model_action(next_state)\n",
    "\n",
    "    #评估next_state的价值\n",
    "    #[b, 4],[b, 1] -> [b, 1]\n",
    "    target1 = model_value_next1(next_state)\n",
    "    target2 = model_value_next2(next_state)\n",
    "\n",
    "    #取价值小的,这是出于稳定性考虑\n",
    "    #[b, 1]\n",
    "    target = torch.min(target1, target2)\n",
    "\n",
    "    # 改5：二维的value和prob结合求其期望\n",
    "    target = (prob * target)\n",
    "    target = target.sum(dim=1, keepdim=True)\n",
    "\n",
    "    #exp和log互为反操作,这里是把alpha还原了\n",
    "    #这里的操作是在target上加上了动作的熵,alpha作为权重系数\n",
    "    #[b, 1] - [b, 1] -> [b, 1]\n",
    "    target += alpha.exp() * entropy\n",
    "\n",
    "    #[b, 1]\n",
    "    target *= 0.99\n",
    "    target *= (1 - over)\n",
    "    target += reward\n",
    "\n",
    "    return target\n",
    "\n",
    "\n",
    "get_target(reward, next_state, over).shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "改3：熵求和后变为1x1的了，value也要加权求和\n",
    "代码位置：get_loss_action 函数中，value按动作概率加权求和。\n",
    "修改意义：\n",
    "\n",
    "背景：SAC算法中，动作策略的损失函数需要考虑状态-动作对的价值，而价值函数的输出需要与动作概率结合，计算期望价值。\n",
    "作用：\n",
    "\n",
    "原代码可能未正确对value进行加权，导致动作策略的损失函数无法准确反映动作分布的期望价值。\n",
    "修改后，value按动作概率加权（value *= prob; value = value.sum(dim=1)），计算的是动作策略的期望价值，这与SAC算法中策略优化的目标一致，即优化 $ \\mathbb{E}_{\\pi(a|s)}[Q(s,a) - \\alpha \\log \\pi(a|s)] $。\n",
    "\n",
    "\n",
    "效果：加权求和使动作策略的损失更贴近理论目标，提升了策略优化的准确性，促进了收敛。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.0309, grad_fn=<MeanBackward0>),\n",
       " tensor([[0.6903],\n",
       "         [0.6882],\n",
       "         [0.6330],\n",
       "         [0.6345],\n",
       "         [0.6858],\n",
       "         [0.6683],\n",
       "         [0.6068],\n",
       "         [0.6033],\n",
       "         [0.6917],\n",
       "         [0.6783],\n",
       "         [0.6314],\n",
       "         [0.6772],\n",
       "         [0.6876],\n",
       "         [0.6669],\n",
       "         [0.6926],\n",
       "         [0.6848],\n",
       "         [0.6860],\n",
       "         [0.6385],\n",
       "         [0.6508],\n",
       "         [0.6520],\n",
       "         [0.6921],\n",
       "         [0.6507],\n",
       "         [0.6920],\n",
       "         [0.5766],\n",
       "         [0.6880],\n",
       "         [0.6924],\n",
       "         [0.6198],\n",
       "         [0.5522],\n",
       "         [0.6852],\n",
       "         [0.6649],\n",
       "         [0.6804],\n",
       "         [0.6740],\n",
       "         [0.6761],\n",
       "         [0.6493],\n",
       "         [0.6751],\n",
       "         [0.6756],\n",
       "         [0.6658],\n",
       "         [0.6787],\n",
       "         [0.6769],\n",
       "         [0.6892],\n",
       "         [0.6098],\n",
       "         [0.6632],\n",
       "         [0.6858],\n",
       "         [0.6761],\n",
       "         [0.6895],\n",
       "         [0.6772],\n",
       "         [0.6619],\n",
       "         [0.6907],\n",
       "         [0.6874],\n",
       "         [0.6920],\n",
       "         [0.6887],\n",
       "         [0.6837],\n",
       "         [0.6930],\n",
       "         [0.6848],\n",
       "         [0.6929],\n",
       "         [0.6923],\n",
       "         [0.6929],\n",
       "         [0.5759],\n",
       "         [0.6918],\n",
       "         [0.6327],\n",
       "         [0.6778],\n",
       "         [0.6652],\n",
       "         [0.6023],\n",
       "         [0.6880]], grad_fn=<ViewBackward0>))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_loss_action(state):\n",
    "    #计算action和熵\n",
    "    #[b, 4] -> [b, 1],[b, 1]\n",
    "    action, entropy, prob = model_action(state)\n",
    "\n",
    "    #使用两个value网络评估action的价值\n",
    "    #[b, 4],[b, 1] -> [b, 1]\n",
    "    value1 = model_value1(state)\n",
    "    value2 = model_value2(state)\n",
    "\n",
    "    #取价值小的,出于稳定性考虑\n",
    "    #[b, 1]\n",
    "    value = torch.min(value1, value2)\n",
    "\n",
    "    #alpha还原后乘以熵,这个值期望的是越大越好,但是这里是计算loss,所以符号取反\n",
    "    #[1] - [b, 1] -> [b, 1]\n",
    "    loss_action = -alpha.exp() * entropy\n",
    "\n",
    "    # 改3：熵求和后变为1x1的了，value也要加权求和\n",
    "    #按概率对value进行加权\n",
    "    value *= prob\n",
    "    value = value.sum(dim=1, keepdim=True)\n",
    "    #减去value,所以value越大越好,这样loss就会越小\n",
    "    loss_action -= value #图片公式里用了V，这里没用到\n",
    "\n",
    "    return loss_action.mean(), entropy\n",
    "\n",
    "\n",
    "get_loss_action(state)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "改6：取对应动作的value值 [b,2]->[b,1]\n",
    "代码位置：train 函数中，价值损失计算时使用gather提取对应动作的Q值。\n",
    "\n",
    "修改意义：\n",
    "\n",
    "背景：价值网络的损失函数需要比较预测的Q值与目标Q值，而预测的Q值需要对应实际采取的动作。\n",
    "作用：\n",
    "原代码未能正确匹配实际动作。\n",
    "修改后，使用gather函数根据实际动作索引（action）提取对应的Q值，确保价值损失计算的是实际执行动作的Q值与目标的差异。\n",
    "效果：这一修改提高了价值网络的训练精度，使其更准确地逼近真实Q值。\n",
    "\n",
    "改7：学习率大小\n",
    "代码位置：train 函数中，调整优化器的学习率。\n",
    "\n",
    "修改意义：\n",
    "\n",
    "背景：学习率是优化器的重要超参数，过高可能导致训练不稳定，过低可能导致收敛过慢。SAC算法通常需要为动作网络和价值网络设置不同的学习率。\n",
    "作用：\n",
    "动作网络的学习率设为1e-3，价值网络为1e-2，alpha为1e-2，这些值是直接复制参考代码的值。\n",
    "合理的学习率平衡了动作策略和价值函数的更新速度，避免某一部分更新过快导致训练失衡。\n",
    "效果：优化学习率提高了训练的稳定性，加速了收敛。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 412 0.002476826310157776 16.3\n",
      "10 3143 0.00010455201845616102 180.9\n",
      "20 5968 2.707117164391093e-05 117.5\n",
      "30 8470 9.042170859174803e-06 167.9\n",
      "40 10000 3.2324410312867258e-06 117.2\n",
      "50 10000 1.187405700875388e-06 134.8\n",
      "60 10000 4.399724389259063e-07 197.9\n",
      "70 10000 1.6337584440861974e-07 106.8\n",
      "80 10000 5.87634403359516e-08 167.2\n",
      "90 10000 2.1969018604295343e-08 200.0\n"
     ]
    }
   ],
   "source": [
    "def train():\n",
    "    #改7： 学习率大小\n",
    "    optimizer_action = torch.optim.Adam(model_action.parameters(), lr=1e-3)\n",
    "    optimizer_value1 = torch.optim.Adam(model_value1.parameters(), lr=1e-2)\n",
    "    optimizer_value2 = torch.optim.Adam(model_value2.parameters(), lr=1e-2)\n",
    "\n",
    "    #alpha也是要更新的参数,所以这里要定义优化器\n",
    "    optimizer_alpha = torch.optim.Adam([alpha], lr=1e-2)\n",
    "\n",
    "    loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "    #训练N次\n",
    "    for epoch in range(100):\n",
    "        #更新N条数据\n",
    "        update_data()\n",
    "\n",
    "        #每次更新过数据后,学习N次\n",
    "        for i in range(200):\n",
    "            #采样一批数据\n",
    "            state, action, reward, next_state, over = get_sample()\n",
    "\n",
    "            #对reward偏移,为了便于训练\n",
    "            reward = (reward + 8) / 8\n",
    "\n",
    "            #计算target,这个target里已经考虑了动作的熵\n",
    "            #[b, 1]\n",
    "            target = get_target(reward, next_state, over)\n",
    "            target = target.detach()\n",
    "\n",
    "            # 改6：取对应动作的value值 [b,2]->[b,1]\n",
    "            #计算两个value\n",
    "            value1 = model_value1(state).gather(dim=1, index=action.to(torch.int64))\n",
    "            value2 = model_value2(state).gather(dim=1, index=action.to(torch.int64))\n",
    "\n",
    "            #计算两个loss,两个value的目标都是要贴近target\n",
    "            loss_value1 = loss_fn(value1, target)\n",
    "            loss_value2 = loss_fn(value2, target)\n",
    "\n",
    "            #更新参数\n",
    "            optimizer_value1.zero_grad()\n",
    "            loss_value1.backward()\n",
    "            optimizer_value1.step()\n",
    "\n",
    "            optimizer_value2.zero_grad()\n",
    "            loss_value2.backward()\n",
    "            optimizer_value2.step()\n",
    "\n",
    "            #使用model_value计算model_action的loss\n",
    "            loss_action, entropy = get_loss_action(state)\n",
    "            optimizer_action.zero_grad()\n",
    "            loss_action.backward()\n",
    "            optimizer_action.step()\n",
    "\n",
    "            #熵乘以alpha就是alpha的loss\n",
    "            #[b, 1] -> [1]\n",
    "            loss_alpha = (entropy + 1).detach() * alpha.exp()\n",
    "            loss_alpha = loss_alpha.mean()\n",
    "\n",
    "            #更新alpha值\n",
    "            optimizer_alpha.zero_grad()\n",
    "            loss_alpha.backward()\n",
    "            optimizer_alpha.step()\n",
    "\n",
    "            #增量更新next模型\n",
    "            soft_update(model_value1, model_value_next1)\n",
    "            soft_update(model_value2, model_value_next2)\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            test_result = sum([test(play=False) for _ in range(10)]) / 10\n",
    "            print(epoch, len(datas), alpha.exp().item(), test_result)\n",
    "\n",
    "\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "200.0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test(play=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL-py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
