{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda \n",
      "Selected GPU index: 0, GPU name: NVIDIA GeForce RTX 2080 Ti\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-0.03230159,  0.02861641, -0.03100644,  0.03228856], dtype=float32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gym\n",
    "import os\n",
    "import torch\n",
    "\n",
    "# 设置使用特定的GPU（例如选择第0张GPU）\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"  # 限制可见的GPU为第0张\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device} \")\n",
    "# 验证选中的GPU\n",
    "if torch.cuda.is_available():\n",
    "    current_device = torch.cuda.current_device()\n",
    "    device_name = torch.cuda.get_device_name(current_device)\n",
    "    print(f\"Selected GPU index: {current_device}, GPU name: {device_name}\")\n",
    "else:\n",
    "    print(\"No GPU available, using CPU\")\n",
    "\n",
    "#定义环境\n",
    "class MyWrapper(gym.Wrapper):\n",
    "\n",
    "    def __init__(self):\n",
    "        env = gym.make('CartPole-v1', render_mode='rgb_array')\n",
    "        super().__init__(env)\n",
    "        self.env = env\n",
    "        self.step_n = 0\n",
    "\n",
    "    def reset(self):\n",
    "        state, _ = self.env.reset()\n",
    "        self.step_n = 0\n",
    "        return state\n",
    "\n",
    "    def step(self, action):\n",
    "        state, reward, terminated, truncated, info = self.env.step(action)\n",
    "        done = terminated or truncated\n",
    "        self.step_n += 1\n",
    "        if self.step_n >= 200:\n",
    "            done = True\n",
    "        return state, reward, done, info\n",
    "\n",
    "\n",
    "env = MyWrapper()\n",
    "\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "#打印游戏\n",
    "def show():\n",
    "    plt.imshow(env.render())\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "改2：离散熵用求和\n",
    "代码位置：ModelAction 类中，计算动作熵的方式从单一动作的负对数概率改为基于整个动作分布的熵求和。\n",
    "修改意义：\n",
    "\n",
    "背景：SAC算法的核心是最大化策略的熵，以鼓励探索。熵的计算需要考虑整个动作分布的概率，而不仅仅是采样动作的概率。离散动作空间的熵公式为：$ H(\\pi) = -\\sum_a \\pi(a|s) \\log \\pi(a|s) $。\n",
    "作用：\n",
    "\n",
    "原代码仅计算采样动作的熵（entropy = -log_prob），忽略了动作分布的整体特性，可能导致熵估计偏低，探索不足。\n",
    "修改后使用整个动作分布的熵公式（entropy = -prob * torch.log(prob + 1e-8).sum(dim=1)），更准确地反映了策略的随机性，符合SAC算法的熵正则化目标。\n",
    "\n",
    "\n",
    "效果：更准确的熵计算增强了探索能力，使策略更倾向于尝试不同动作，从而避免过早收敛到次优解。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "\n",
    "\n",
    "class ModelAction(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = torch.nn.Sequential(\n",
    "                    torch.nn.Linear(4, 128),\n",
    "                    torch.nn.ReLU(),\n",
    "                    torch.nn.Linear(128, 2),\n",
    "                    torch.nn.Softmax(dim=1),\n",
    "                )\n",
    "\n",
    "    def forward(self, state):\n",
    "        prob = self.model(state)  \n",
    "\n",
    "        # 采样动作\n",
    "        action = [random.choices(range(2), weights=p.tolist(), k=1)[0] for p in prob]  # 采样一个动作\n",
    "\n",
    "        # 计算熵\n",
    "        # prob[0, action] 是采样动作 action 的概率\n",
    "        #log_prob = torch.log(prob + 1e-7)  # 避免 log(0) 加小 epsilon\n",
    "        #entropy = -log_prob  # 单个动作的熵贡献，-log π(a|s)\n",
    "\n",
    "        # 改2：离散熵用求和\n",
    "        #[b, 2]\n",
    "        entropy = prob * torch.log(prob + 1e-8)\n",
    "\n",
    "        #所有动作的熵求和\n",
    "        #[b, 2] -> [b, 1]\n",
    "        entropy = -entropy.sum(dim=1, keepdim=True)\n",
    "        # 或者更准确的熵估计（基于整个分布）\n",
    "        # 熵 = -∑ π(a|s) log π(a|s)，可以使用 prob 直接计算期望\n",
    "        #entropy = -torch.sum(prob * torch.log(prob + 1e-7))  # 整个分布的熵\n",
    "\n",
    "        return action, entropy.reshape(-1,1), prob\n",
    "\n",
    "\n",
    "model_action = ModelAction()\n",
    "model_action = ModelAction().to(device)\n",
    "\n",
    "#model_action(torch.randn(2,4))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "改4：输出是二维以匹配后面的与prob相乘\n",
    "代码位置：ModelValue 类中，价值网络输出从1维改为2维。\n",
    "修改意义：\n",
    "背景：在离散动作空间的SAC中，价值网络需要为每个动作输出一个Q值（对应于动作空间的维度）。CartPole-v1的动作空间是离散的（**2个动作：左或右**），因此价值网络的输出需要是2维，以匹配动作概率的维度。\n",
    "作用：\n",
    "原代码中价值网络输出可能是1维，无法与动作概率（2维）进行点乘或加权操作。\n",
    "修改后，价值网络输出2维Q值（对应两个动作），可以与动作概率分布（prob）逐元素相乘，计算期望价值。\n",
    "效果：这一修改使价值网络的输出与动作概率分布兼容，保证了策略损失和价值损失的计算正确性。\n",
    "\n",
    "\n",
    "改9：删去一层\n",
    "代码位置：ModelValue 类中，删去价值网络中的一层隐藏层。\n",
    "修改意义：\n",
    "背景：神经网络的深度和复杂度需要与任务的复杂性匹配。CartPole-v1是一个相对简单的环境，过深的网络可能导致过拟合或训练困难。\n",
    "作用：\n",
    "删除一层隐藏层（从多层简化为两层：Linear(4, 128) -> ReLU -> Linear(128, 2)），降低了价值网络的复杂度，减少了过拟合风险。\n",
    "简化网络结构可能使梯度传播更稳定，适合CartPole-v1的简单状态空间（4维）。\n",
    "效果：减少网络复杂性提高了训练效率和稳定性，适合任务需求。\n",
    "**实际运行时，多加的这一层反而导致训练效果变差**\n",
    "\n",
    "改10：去掉动作输入\n",
    "代码位置：ModelValue 类中，去掉动作输入，使价值网络仅接受状态输入。\n",
    "修改意义：\n",
    "背景：SAC算法中，价值网络（Q网络）通常为每个动作输出一个Q值（即$ Q(s,a) $），但在离散动作空间中，Q网络可以设计为仅接受状态输入，输出所有动作的Q值向量。这种设计在CartPole-v1（动作空间为2）中是可行的。\n",
    "作用：\n",
    "原代码中，价值网络将状态和动作拼接作为输入（例如输入维度为5，可能包含4维状态+1维动作）。这要求网络为每个状态-动作对单独预测Q值，增加了模型复杂度，且可能引入不必要的噪声，因为动作信息可以通过输出维度隐式建模。\n",
    "修改后，价值网络仅接受4维状态输入，输出2维Q值（对应两个动作），简化了网络结构，直接预测所有动作的Q值。\n",
    "效果：这一修改降低了模型的输入复杂性，使价值网络更专注于学习状态到Q值的映射，减少了过拟合风险，提高了训练稳定性。\n",
    "**加上动作输入后，训练效果其差**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([1, 1],\n",
       " tensor([[0.6931],\n",
       "         [0.6921]], device='cuda:0', grad_fn=<ViewBackward0>),\n",
       " tensor([[0.4982, 0.5018],\n",
       "         [0.4773, 0.5227]], device='cuda:0', grad_fn=<SoftmaxBackward0>))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class ModelValue(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.sequential = torch.nn.Sequential(\n",
    "            torch.nn.Linear(4, 128),#改10：去掉动作输入\n",
    "            torch.nn.ReLU(),\n",
    "            #torch.nn.Linear(128, 128),\n",
    "            #torch.nn.ReLU(),#改9：删去一层\n",
    "            # 改4：输出是二维以匹配后面的与prob相乘\n",
    "            torch.nn.Linear(128, 2),\n",
    "        )\n",
    "\n",
    "    def forward(self, state):\n",
    "        #[b, 5] -> [b, 1]\n",
    "        return self.sequential(state)\n",
    "\n",
    "\n",
    "model_value1 = ModelValue()\n",
    "model_value2 = ModelValue()\n",
    "\n",
    "model_value_next1 = ModelValue()\n",
    "model_value_next2 = ModelValue()\n",
    "\n",
    "# 初始化模型并移动到GPU\n",
    "model_value1 = ModelValue().to(device)\n",
    "model_value2 = ModelValue().to(device)\n",
    "model_value_next1 = ModelValue().to(device)\n",
    "model_value_next2 = ModelValue().to(device)\n",
    "\n",
    "model_value_next1.load_state_dict(model_value1.state_dict())\n",
    "model_value_next2.load_state_dict(model_value2.state_dict())\n",
    "\n",
    "# 测试模型\n",
    "model_value1(torch.randn(2, 4).to(device))\n",
    "model_action(torch.randn(2, 4).to(device))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def get_action(state):\n",
    "    state = torch.FloatTensor(state).reshape(1, 4).to(device)\n",
    "    #[1, 4] -> [1, 2]\n",
    "    action, _, __ = model_action(state)\n",
    "\n",
    "    return action\n",
    "\n",
    "\n",
    "get_action([1, 2, 3, 4])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "改1：要收集足够多的数据\n",
    "代码位置：update_data 函数中，循环收集至少200条数据，并限制样本池大小为10000。\n",
    "\n",
    "修改意义：\n",
    "\n",
    "背景：SAC是一种基于经验回放（Replay Buffer）的强化学习算法，需要从环境中收集足够多的数据样本以保证策略和价值函数的训练稳定性。样本池过小会导致模型学习到的策略缺乏多样性，容易陷入局部最优。\n",
    "作用：\n",
    "通过确保每次更新至少收集200条新数据，增加了数据的多样性，有助于模型探索不同的状态-动作对，改善策略的泛化能力。\n",
    "限制样本池最大为10000条，避免内存占用过大，同时通过删除最旧的数据保持样本的新鲜度，适应环境动态变化。\n",
    "效果：这一修改保证了训练过程中有足够的数据支持，减少了过拟合的风险，提升了模型的稳定性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yuchen/miniconda3/envs/RL-py39/lib/python3.9/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((209, 0), 209)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#样本池\n",
    "datas = []\n",
    "\n",
    "\n",
    "### 改1：要收集足够多的数据\n",
    "#向样本池中添加N条数据,删除M条最古老的数据\n",
    "def update_data():\n",
    "    old_count = len(datas)\n",
    "\n",
    "    #玩到新增了N个数据为止\n",
    "    while len(datas) - old_count < 200:\n",
    "        #初始化游戏\n",
    "        state = env.reset()\n",
    "\n",
    "        #玩到游戏结束为止\n",
    "        over = False\n",
    "        while not over:\n",
    "            #根据当前状态得到一个动作\n",
    "            action = get_action(state)\n",
    "\n",
    "            #执行动作,得到反馈\n",
    "            next_state, reward, over, _ = env.step(action[0])\n",
    "\n",
    "            #记录数据样本\n",
    "            datas.append((state, action, reward, next_state, over))\n",
    "\n",
    "            #更新游戏状态,开始下一个动作\n",
    "            state = next_state\n",
    "\n",
    "    update_count = len(datas) - old_count\n",
    "    drop_count = max(len(datas) - 10000, 0)\n",
    "\n",
    "    #数据上限,超出时从最古老的开始删除\n",
    "    while len(datas) > 10000:\n",
    "        datas.pop(0)\n",
    "\n",
    "    return update_count, drop_count\n",
    "\n",
    "\n",
    "update_data(), len(datas)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "改8： 返回数据类型 action F->L reward:L->F\n",
    "类型没改好，动作只有0,1所以是Long整形，reward是奖励用浮点数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9604/2539608082.py:8: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)\n",
      "  state = torch.FloatTensor([i[0] for i in samples]).reshape(-1, 4).to(device)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.0019, -0.7830,  0.1021,  1.3370],\n",
       "         [-0.0334, -0.7859,  0.1620,  1.4098],\n",
       "         [-0.0329, -0.5705,  0.1447,  1.0444],\n",
       "         [ 0.0373, -0.1792,  0.0140,  0.3150],\n",
       "         [-0.0190, -0.0239,  0.0488, -0.0079]], device='cuda:0'),\n",
       " tensor([[0],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1]], device='cuda:0'),\n",
       " tensor([[1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.]], device='cuda:0'),\n",
       " tensor([[-0.0138, -0.9793,  0.1288,  1.6598],\n",
       "         [-0.0491, -0.5931,  0.1902,  1.1718],\n",
       "         [-0.0444, -0.3776,  0.1656,  0.8004],\n",
       "         [ 0.0337,  0.0157,  0.0203,  0.0267],\n",
       "         [-0.0195,  0.1705,  0.0486, -0.2848]], device='cuda:0'),\n",
       " tensor([[0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0]], device='cuda:0'))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#获取一批数据样本\n",
    "def get_sample():\n",
    "    #从样本池中采样\n",
    "    samples = random.sample(datas, 64)\n",
    "\n",
    "    # 改8： 返回数据类型 action F->L reward:L->F\n",
    "    #[b, 4]\n",
    "    state = torch.FloatTensor([i[0] for i in samples]).reshape(-1, 4).to(device)\n",
    "    #[b, 1]\n",
    "    action = torch.LongTensor([i[1] for i in samples]).reshape(-1, 1).to(device)\n",
    "    #[b, 1]\n",
    "    reward = torch.FloatTensor([i[2] for i in samples]).reshape(-1, 1).to(device)\n",
    "    #[b, 4]\n",
    "    next_state = torch.FloatTensor([i[3] for i in samples]).reshape(-1, 4).to(device)\n",
    "    #[b, 1]\n",
    "    over = torch.LongTensor([i[4] for i in samples]).reshape(-1, 1).to(device)\n",
    "\n",
    "    return state, action, reward, next_state, over\n",
    "\n",
    "\n",
    "state, action, reward, next_state, over = get_sample()\n",
    "\n",
    "state[:5], action[:5], reward[:5], next_state[:5], over[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython import display\n",
    "\n",
    "\n",
    "def test(play):\n",
    "    #初始化游戏\n",
    "    state = env.reset()\n",
    "\n",
    "    #记录反馈值的和,这个值越大越好\n",
    "    reward_sum = 0\n",
    "\n",
    "    #玩到游戏结束为止\n",
    "    over = False\n",
    "    while not over:\n",
    "        #根据当前状态得到一个动作\n",
    "        action = get_action(state)\n",
    "\n",
    "        #执行动作,得到反馈\n",
    "        state, reward, over, _ = env.step(action[0])\n",
    "        reward_sum += reward\n",
    "\n",
    "        #打印动画\n",
    "        if play and random.random() < 0.2:  #跳帧\n",
    "            display.clear_output(wait=True)\n",
    "            show()\n",
    "\n",
    "    return reward_sum\n",
    "\n",
    "\n",
    "test(play=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def soft_update(model, model_next):\n",
    "    for param, param_next in zip(model.parameters(), model_next.parameters()):\n",
    "        #以一个小的比例更新\n",
    "        value = param_next.data * 0.995 + param.data * 0.005\n",
    "        param_next.data.copy_(value)\n",
    "\n",
    "\n",
    "soft_update(torch.nn.Linear(4, 64), torch.nn.Linear(4, 64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-4.6052, device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "#这也是一个可学习的参数\n",
    "alpha = torch.tensor(math.log(0.01), device=device)\n",
    "alpha.requires_grad = True\n",
    "\n",
    "alpha"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "改5：二维的value和prob结合求其期望\n",
    "代码位置：get_target 函数中，target价值按动作概率加权求和。\n",
    "修改意义：\n",
    "\n",
    "背景：SAC算法的目标价值（target）需要考虑下一状态的动作分布，计算期望Q值并加上熵正则化项。公式为：$ \\mathbb{E}_{\\pi(a'|s')}[Q(s',a') - \\alpha \\log \\pi(a'|s')] $。\n",
    "作用：\n",
    "\n",
    "原代码直接使用单一动作的Q值，忽略了动作分布的期望。\n",
    "修改后，通过将价值（target）与动作概率（prob）相乘并求和，计算下一状态的期望Q值，并加上熵项（target += alpha.exp() * entropy）。\n",
    "\n",
    "\n",
    "效果：这一修改确保了目标价值的计算符合SAC算法的理论框架，增强了训练的稳定性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 1])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_target(reward, next_state, over):\n",
    "    #首先使用model_action计算动作和动作的熵\n",
    "    #[b, 4] -> [b, 1],[b, 1]\n",
    "    action, entropy, prob= model_action(next_state)\n",
    "\n",
    "    #评估next_state的价值\n",
    "    #[b, 4],[b, 1] -> [b, 1]\n",
    "    target1 = model_value_next1(next_state)\n",
    "    target2 = model_value_next2(next_state)\n",
    "\n",
    "    #取价值小的,这是出于稳定性考虑\n",
    "    #[b, 1]\n",
    "    target = torch.min(target1, target2)\n",
    "\n",
    "    # 改5：二维的value和prob结合求其期望\n",
    "    target = (prob * target)\n",
    "    target = target.sum(dim=1, keepdim=True)\n",
    "\n",
    "    #exp和log互为反操作,这里是把alpha还原了\n",
    "    #这里的操作是在target上加上了动作的熵,alpha作为权重系数\n",
    "    #[b, 1] - [b, 1] -> [b, 1]\n",
    "    target += alpha.exp() * entropy\n",
    "\n",
    "    #[b, 1]\n",
    "    target *= 0.99\n",
    "    target *= (1 - over)\n",
    "    target += reward\n",
    "\n",
    "    return target\n",
    "\n",
    "\n",
    "get_target(reward, next_state, over).shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "改3：熵求和后变为1x1的了，value也要加权求和\n",
    "代码位置：get_loss_action 函数中，value按动作概率加权求和。\n",
    "修改意义：\n",
    "\n",
    "背景：SAC算法中，动作策略的损失函数需要考虑状态-动作对的价值，而价值函数的输出需要与动作概率结合，计算期望价值。\n",
    "作用：\n",
    "\n",
    "原代码可能未正确对value进行加权，导致动作策略的损失函数无法准确反映动作分布的期望价值。\n",
    "修改后，value按动作概率加权（value *= prob; value = value.sum(dim=1)），计算的是动作策略的期望价值，这与SAC算法中策略优化的目标一致，即优化 $ \\mathbb{E}_{\\pi(a|s)}[Q(s,a) - \\alpha \\log \\pi(a|s)] $。\n",
    "\n",
    "\n",
    "效果：加权求和使动作策略的损失更贴近理论目标，提升了策略优化的准确性，促进了收敛。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.2542, device='cuda:0', grad_fn=<MeanBackward0>),\n",
       " tensor([[0.6912],\n",
       "         [0.6910],\n",
       "         [0.6913],\n",
       "         [0.6915],\n",
       "         [0.6925],\n",
       "         [0.6931],\n",
       "         [0.6913],\n",
       "         [0.6931],\n",
       "         [0.6906],\n",
       "         [0.6930],\n",
       "         [0.6931],\n",
       "         [0.6912],\n",
       "         [0.6931],\n",
       "         [0.6930],\n",
       "         [0.6931],\n",
       "         [0.6929],\n",
       "         [0.6926],\n",
       "         [0.6931],\n",
       "         [0.6927],\n",
       "         [0.6926],\n",
       "         [0.6931],\n",
       "         [0.6930],\n",
       "         [0.6931],\n",
       "         [0.6910],\n",
       "         [0.6930],\n",
       "         [0.6919],\n",
       "         [0.6921],\n",
       "         [0.6931],\n",
       "         [0.6930],\n",
       "         [0.6931],\n",
       "         [0.6931],\n",
       "         [0.6912],\n",
       "         [0.6918],\n",
       "         [0.6930],\n",
       "         [0.6930],\n",
       "         [0.6931],\n",
       "         [0.6926],\n",
       "         [0.6912],\n",
       "         [0.6920],\n",
       "         [0.6931],\n",
       "         [0.6930],\n",
       "         [0.6927],\n",
       "         [0.6930],\n",
       "         [0.6930],\n",
       "         [0.6913],\n",
       "         [0.6930],\n",
       "         [0.6931],\n",
       "         [0.6921],\n",
       "         [0.6916],\n",
       "         [0.6930],\n",
       "         [0.6931],\n",
       "         [0.6914],\n",
       "         [0.6930],\n",
       "         [0.6925],\n",
       "         [0.6908],\n",
       "         [0.6931],\n",
       "         [0.6927],\n",
       "         [0.6914],\n",
       "         [0.6916],\n",
       "         [0.6925],\n",
       "         [0.6925],\n",
       "         [0.6912],\n",
       "         [0.6922],\n",
       "         [0.6925]], device='cuda:0', grad_fn=<ViewBackward0>))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_loss_action(state):\n",
    "    #计算action和熵\n",
    "    #[b, 4] -> [b, 1],[b, 1]\n",
    "    action, entropy, prob = model_action(state)\n",
    "\n",
    "    #使用两个value网络评估action的价值\n",
    "    #[b, 4],[b, 1] -> [b, 1]\n",
    "    value1 = model_value1(state)\n",
    "    value2 = model_value2(state)\n",
    "\n",
    "    #取价值小的,出于稳定性考虑\n",
    "    #[b, 1]\n",
    "    value = torch.min(value1, value2)\n",
    "\n",
    "    #alpha还原后乘以熵,这个值期望的是越大越好,但是这里是计算loss,所以符号取反\n",
    "    #[1] - [b, 1] -> [b, 1]\n",
    "    loss_action = -alpha.exp() * entropy\n",
    "\n",
    "    # 改3：熵求和后变为1x1的了，value也要加权求和\n",
    "    #按概率对value进行加权\n",
    "    value *= prob\n",
    "    value = value.sum(dim=1, keepdim=True)\n",
    "    #减去value,所以value越大越好,这样loss就会越小\n",
    "    loss_action -= value #图片公式里用了V，这里没用到\n",
    "\n",
    "    return loss_action.mean(), entropy\n",
    "\n",
    "\n",
    "get_loss_action(state)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "改6：取对应动作的value值 [b,2]->[b,1]\n",
    "代码位置：train 函数中，价值损失计算时使用gather提取对应动作的Q值。\n",
    "\n",
    "修改意义：\n",
    "\n",
    "背景：价值网络的损失函数需要比较预测的Q值与目标Q值，而预测的Q值需要对应实际采取的动作。\n",
    "作用：\n",
    "原代码未能正确匹配实际动作。\n",
    "修改后，使用gather函数根据实际动作索引（action）提取对应的Q值，确保价值损失计算的是实际执行动作的Q值与目标的差异。\n",
    "效果：这一修改提高了价值网络的训练精度，使其更准确地逼近真实Q值。\n",
    "\n",
    "改7：学习率大小\n",
    "代码位置：train 函数中，调整优化器的学习率。\n",
    "\n",
    "修改意义：\n",
    "\n",
    "背景：学习率是优化器的重要超参数，过高可能导致训练不稳定，过低可能导致收敛过慢。SAC算法通常需要为动作网络和价值网络设置不同的学习率。\n",
    "作用：\n",
    "动作网络的学习率设为1e-3，价值网络为1e-2，alpha为1e-2，这些值是直接复制参考代码的值。\n",
    "合理的学习率平衡了动作策略和价值函数的更新速度，避免某一部分更新过快导致训练失衡。\n",
    "效果：优化学习率提高了训练的稳定性，加速了收敛。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 411 0.002760456409305334 10.3\n",
      "10 2677 0.00010094821482198313 191.6\n",
      "20 5598 2.472913001838606e-05 163.5\n",
      "30 8715 8.06658499641344e-06 191.4\n",
      "40 10000 2.9141745017113863e-06 176.2\n",
      "50 10000 1.0631489431034424e-06 196.2\n",
      "60 10000 3.9388544337271014e-07 181.1\n",
      "70 10000 1.4747030263606575e-07 181.2\n",
      "80 10000 5.5894361139507964e-08 162.9\n",
      "90 10000 2.073456961682041e-08 196.6\n"
     ]
    }
   ],
   "source": [
    "def train():\n",
    "    #改7： 学习率大小\n",
    "    optimizer_action = torch.optim.Adam(model_action.parameters(), lr=1e-3)\n",
    "    optimizer_value1 = torch.optim.Adam(model_value1.parameters(), lr=1e-2)\n",
    "    optimizer_value2 = torch.optim.Adam(model_value2.parameters(), lr=1e-2)\n",
    "\n",
    "    #alpha也是要更新的参数,所以这里要定义优化器\n",
    "    optimizer_alpha = torch.optim.Adam([alpha], lr=1e-2)\n",
    "\n",
    "    loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "    #训练N次\n",
    "    for epoch in range(100):\n",
    "        #更新N条数据\n",
    "        update_data()\n",
    "\n",
    "        #每次更新过数据后,学习N次\n",
    "        for i in range(200):\n",
    "            #采样一批数据\n",
    "            state, action, reward, next_state, over = get_sample()\n",
    "\n",
    "            #对reward偏移,为了便于训练\n",
    "            reward = (reward + 8) / 8\n",
    "\n",
    "            #计算target,这个target里已经考虑了动作的熵\n",
    "            #[b, 1]\n",
    "            target = get_target(reward, next_state, over)\n",
    "            target = target.detach()\n",
    "\n",
    "            # 改6：取对应动作的value值 [b,2]->[b,1]\n",
    "            #计算两个value\n",
    "            value1 = model_value1(state).gather(dim=1, index=action.to(torch.int64))\n",
    "            value2 = model_value2(state).gather(dim=1, index=action.to(torch.int64))\n",
    "\n",
    "            #计算两个loss,两个value的目标都是要贴近target\n",
    "            loss_value1 = loss_fn(value1, target)\n",
    "            loss_value2 = loss_fn(value2, target)\n",
    "\n",
    "            #更新参数\n",
    "            optimizer_value1.zero_grad()\n",
    "            loss_value1.backward()\n",
    "            optimizer_value1.step()\n",
    "\n",
    "            optimizer_value2.zero_grad()\n",
    "            loss_value2.backward()\n",
    "            optimizer_value2.step()\n",
    "\n",
    "            #使用model_value计算model_action的loss\n",
    "            loss_action, entropy = get_loss_action(state)\n",
    "            optimizer_action.zero_grad()\n",
    "            loss_action.backward()\n",
    "            optimizer_action.step()\n",
    "\n",
    "            #熵乘以alpha就是alpha的loss\n",
    "            #[b, 1] -> [1]\n",
    "            loss_alpha = (entropy + 1).detach() * alpha.exp()\n",
    "            loss_alpha = loss_alpha.mean()\n",
    "\n",
    "            #更新alpha值\n",
    "            optimizer_alpha.zero_grad()\n",
    "            loss_alpha.backward()\n",
    "            optimizer_alpha.step()\n",
    "\n",
    "            #增量更新next模型\n",
    "            soft_update(model_value1, model_value_next1)\n",
    "            soft_update(model_value2, model_value_next2)\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            test_result = sum([test(play=False) for _ in range(10)]) / 10\n",
    "            print(epoch, len(datas), alpha.exp().item(), test_result)\n",
    "\n",
    "\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "200.0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test(play=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL-py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
